import numpy as np
from collections import deque
from math import ceil, log2


# Algorithm requires a maxflow algorithm; we implement Dinic/Dinitz's algorithm here
class Dinic:
    # define an Edge class inside Dinic for convenience; not to be used globally
    class Edge:
        __slots__ = ("v", "rev", "cap")

        def __init__(self, v, rev, cap):
            self.v = v
            self.rev = rev
            self.cap = cap

    def __init__(self, n):
        self.n = n
        self.g = [[] for _ in range(n)]

    def add_edge(self, u, v, cap):
        # forward edge index = len(g[u]), backward edge index = len(g[v])
        self.g[u].append(Dinic.Edge(v, len(self.g[v]), cap))
        self.g[v].append(Dinic.Edge(u, len(self.g[u]) - 1, 0.0))

    def bfs_level(self, s, t, level):
        for i in range(len(level)):
            level[i] = -1
        q = deque()
        level[s] = 0
        q.append(s)
        while q:
            u = q.popleft()
            for e in self.g[u]:
                if e.cap > 0 and level[e.v] < 0:
                    level[e.v] = level[u] + 1
                    if e.v == t:
                        return True
                    q.append(e.v)
        return level[t] >= 0

    def dfs_block(self, u, t, f, level, it):
        if u == t:
            return f
        for i in range(it[u], len(self.g[u])):
            e = self.g[u][i]
            if e.cap > 0 and level[e.v] == level[u] + 1:
                pushed = self.dfs_block(e.v, t, min(f, e.cap), level, it)
                if pushed > 0:
                    e.cap -= pushed
                    self.g[e.v][e.rev].cap += pushed
                    return pushed
            it[u] += 1
        return 0

    def max_flow(self, s, t):
        flow = 0.0
        level = [-1] * self.n
        # repeat BFS and DFS blocking flow
        while self.bfs_level(s, t, level):
            it = [0] * self.n
            pushed = self.dfs_block(s, t, float('inf'), level, it)
            while pushed and pushed > 0:
                flow += pushed
                pushed = self.dfs_block(s, t, float('inf'), level, it)
        return flow

    def mincut_source_side(self, s):
        # After running max_flow, find vertices reachable from s in residual graph
        seen = [False] * self.n
        q = deque([s])
        seen[s] = True
        while q:
            u = q.popleft()
            for e in self.g[u]:
                if e.cap > 1e-12 and not seen[e.v]:
                    seen[e.v] = True
                    q.append(e.v)
        return seen


def _build_dinic_from_adjmat(adj: np.ndarray, include_self_loops=False):
    """
    Build Dinic instance (directed) for an undirected weighted adjacency matrix.
    We'll add both directions for each undirected edge.
    """
    n = adj.shape[0]
    din = Dinic(n)
    for i in range(n):
        for j in range(n):
            if i == j and not include_self_loops:
                continue
            w = float(adj[i, j])
            if w > 0:
                din.add_edge(i, j, w)
    return din


def _build_dinic_with_super_nodes(adj: np.ndarray, Aset, Bset, INF):
    """
    Build graph with super-source (index = n) connected to Aset with INF,
    and super-sink (index = n+1) connected from Bset with INF.
    Return Dinic instance, s_index, t_index.
    """
    n = adj.shape[0]
    din = Dinic(n + 2)
    for i in range(n):
        for j in range(i+1, n):
            w = float(adj[i, j])
            if w > 0:
                # to simulate undirected capacity we add two directed edges
                din.add_edge(i, j, w)
                din.add_edge(j, i, w)
    s = n
    t = n + 1
    for u in Aset:
        din.add_edge(s, u, INF)
    for v in Bset:
        din.add_edge(v, t, INF)
    return din, s, t

def expander_decomposition(graph_matrix: np.ndarray, phi: float, lambda_approx: float, U: list):
    """
    Decomposes the graph into clusters with conductance at least phi.
    Returns a list of clusters (each cluster is a list of vertex indices).
    
    This is a simplified version - for production use, implement Theorem 4.8 (Matula's algorithm) from the paper.
    For now, we use a basic approach: each vertex in U forms its own cluster,
    and vertices not in U are grouped separately.
    """
    n = graph_matrix.shape[0]
    
    # Simple implementation: treat each vertex in U as its own cluster
    # and group all non-U vertices together
    clusters = []
    
    # Each vertex in U becomes a singleton cluster
    for v in U:
        clusters.append([v])
    
    # Group remaining vertices
    remaining = [v for v in range(n) if v not in U]
    if remaining:
        clusters.append(remaining)
    
    return clusters

def sparsify_terminals(clusters, R_curr):
    """
    Selects representatives from each cluster to form a new terminal set R'.
    
    Input:
        clusters: List of clusters from expander decomposition
        R_curr: Current set of terminals
    
    Output:
        R_new: New sparsified terminal set
    """
    Rset = set(R_curr)
    Rprime = []
    for C in clusters:
        inter = [v for v in C if v in Rset]
        if inter:
            rep = min(inter)
            Rprime.append(rep)

    Rprime = sorted(list(dict.fromkeys(Rprime)))
    return Rprime


def isolating_cut(graph_matrix: np.ndarray, R=None) -> float:
    """
    Computes a minimum cut value using the isolating cuts approach (Theorem 2.2).
    Input:
        graph_matrix: (n, n) numpy array adjacency (weights >= 0). Undirected expected.
    Output:
        float: value of the found minimum cut (approx/exact according to flow).
    Note: This returns the min cut value found (not the cut set). Matches requested endpoint.
    """

    if graph_matrix is None:
        raise ValueError("graph_matrix is None")
    if graph_matrix.shape[0] == 0:
        return 0.0
    if graph_matrix.shape[0] != graph_matrix.shape[1]:
        raise ValueError("graph_matrix must be square")
    #Adding check to make sure the graph is undirected
    if not np.allclose(graph_matrix, graph_matrix.T):
        raise ValueError("graph_matrix must be undirected)")
    
    #If R is not provided, set it to all nodes as before
    if R is None:
        R = list(range(graph_matrix.shape[0]))

    #Making sure nothing went wrong when defining R previously
    for r in R:
        if r < 0 or r >= graph_matrix.shape[0]:
            raise ValueError(f"R contains invalid node index: {r}")

    n = graph_matrix.shape[0]

    total_weight = float(np.sum(np.triu(graph_matrix, k=1)))
    if total_weight <= 0:
        return 0.0
    #May need to be larger than this???
    INF = total_weight + 1.0

    if len(R) == 1:
        return 0.0

    bits = max(1, ceil(log2(len(R))))

    sides = []
    for i in range(bits):
        A = [v for v in R if ((v >> i) & 1) == 0]
        B = [v for v in R if ((v >> i) & 1) == 1]

        if len(A) == 0 or len(B) == 0:
            sides.append([True] * (n + 2))
            continue

        din, s, t = _build_dinic_with_super_nodes(graph_matrix, A, B, INF)
        _ = din.max_flow(s, t)
        reachable = din.mincut_source_side(s)
        sides.append(reachable[:n])

    U_list = []
    for v in R:
        Uv_mask = np.ones(n, dtype=bool)
        for i in range(bits):
            side_mask = np.array(sides[i][:n], dtype=bool)
            if side_mask[v]:
                Uv_mask &= side_mask
            else:
                Uv_mask &= ~side_mask
        if not Uv_mask[v]:
            # at least ensure v included
            Uv_mask[v] = True
        U_list.append(Uv_mask)

    #Defining a list of cuts for each Uv to use later in recursion when finding min cut value
    cuts = []
    for Uv_mask in U_list:
        cut_other_side = ~Uv_mask
        cut= np.sum(graph_matrix[Uv_mask][:, cut_other_side])
        cuts.append(cut)

    #Finding the largest of the Uv sets to determine balanced vs unbalanced case
    Uv_sizes = [np.sum(Uv_mask) for Uv_mask in U_list]
    max_size = max(Uv_sizes)

    # Using a threshold of 0.5 to determine if we are in the balanced or unbalanced case
    if max_size <= 0.5 * n:  # 2-balanced case
        # workflow as I understand it:
        # terminal set R
        # sparsify R:
        #   - use expander decomposition (Matula) to get clusters
        #   - each cluster must not have any internal cuts with conductance < phi = 1/polylog(n)
        #   - i.e. if a cluster has conductance < phi, we split it further
        # - pick one representative from each cluster
        # - the representatives form R', which is smaller than R
        # repeat until |R'| < polylog(n) (variable: R_limit)
        # Set phi parameter (conductance threshold)
        C = 8
        n = graph_matrix.shape[0]
        # use natural log; ensure no division by zero
        phi = 1.0 / (max(2.0, np.log(n)) ** C)   # phi = 1/(log n)^C
        # stopping threshold for terminals (polylog(n))
        B = max(1, int(np.ceil((np.log(max(2.0, n))) ** 3)))

        # R is the current terminal set (list)
        R_curr = list(R)   # ensure list

        # iterative sparsification loop: repeat until terminals are polylog-sized
        while len(R_curr) > B:
            # decompose graph into phi-expanders (deterministic)
            clusters = expander_decomposition(graph_matrix, phi)

            # representative per cluster that intersects R_curr
            R_new = sparsify_terminals(clusters, R_curr)

            # fallback to guarantee progress
            if len(R_new) >= len(R_curr):
                # deterministic halving (keep every 2nd element sorted by index)
                # since theorem guarantees at least halving
                R_sorted = sorted(R_curr)
                R_new = R_sorted[::2]
                if len(R_new) == 0:
                    R_new = R_sorted[:1]
            R_curr = R_new
        return isolating_cut(graph_matrix, R_curr)
    else:
        #Handle unbalanced case with recursion
        index_largest_set = np.argmax(Uv_sizes)
        largest_set_mask = U_list[index_largest_set]
        largest_set_cut_value = float(cuts[index_largest_set])

        rest_graph_mask = ~largest_set_mask
        rest_graph_indices = np.nonzero(rest_graph_mask)[0].tolist()

        #If the largest set is the whole graph, return its cut value
        if len(rest_graph_indices) == 0:
            return largest_set_cut_value

        # Build subgraph for the rest of the graph
        rest_adjacencies = graph_matrix[np.ix_(rest_graph_indices, rest_graph_indices)]

        #Determining and storing new set of terminals in the rest of the graph
        idx_map = {orig: new for new, orig in enumerate(rest_graph_indices)}
        R_new = []
        for v in R:
            if v in idx_map:
                R_new.append(idx_map[v])

        # If no terminals remain in subproblem, return cut
        if len(R_new) == 0:
            return largest_set_cut_value

        # Recursively solve inside complement
        rest_graph_min_cut = isolating_cut(rest_adjacencies, R_new)

        #return either the largest set cut value or the rest graph min cut, whichever is smaller
        return min(largest_set_cut_value, rest_graph_min_cut)
